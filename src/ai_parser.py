# src/ai_parser.py

import requests
import json
import re
from lxml import etree
import math

# --- å¸¸é‡å®šä¹‰éƒ¨åˆ†ä¿æŒä¸å˜ ---
OLLAMA_API_URL = "http://localhost:11434/api/chat"
MODEL_NAME = "qwen2.5-coder:14b"
SYSTEM_PROMPT_FILE = "prompts/system_prompt.txt"
LATEX_PROMPT_FILE = "prompts/prompt_for_latex_convert.txt"


# --- translate_latex_to_omml_llm å‡½æ•°ä¿æŒä¸å˜ ---
def translate_latex_to_omml_llm(latex_string: str) -> str | None:
    # ... æ­¤å‡½æ•°å·²æœ‰è¶³å¤Ÿçš„ print è¾“å‡ºï¼Œæ— éœ€ä¿®æ”¹ ...
    print(f"ğŸ¤– å°è¯•ä½¿ç”¨LLMè½¬è¯‘LaTeX: {latex_string}")
    try:
        with open(LATEX_PROMPT_FILE, 'r', encoding='utf-8') as f:
            system_prompt = f.read()
    except FileNotFoundError:
        print(f"âŒ è‡´å‘½é”™è¯¯: æ— æ³•æ‰¾åˆ°LaTeXè½¬æ¢æç¤ºè¯æ–‡ä»¶ -> {LATEX_PROMPT_FILE}")
        return None

    user_prompt = f"""
Convert the following LaTeX formula into a centered OMML `<m:oMathPara>` XML block.
LaTeX Input: `{latex_string}`
Alignment: center
"""

    payload = {
        "model": MODEL_NAME,
        "messages": [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ],
        "stream": False
    }

    try:
        response = requests.post(OLLAMA_API_URL, json=payload, timeout=60)
        response.raise_for_status()
        response_data = response.json()
        omml_xml_string = response_data.get('message', {}).get('content')

        if not omml_xml_string:
            print("âŒ LLMè¿”å›å†…å®¹ä¸ºç©ºã€‚")
            return None

        try:
            omml_xml_string = re.sub(r'^```xml\s*|\s*```$', '', omml_xml_string, flags=re.MULTILINE).strip()
            etree.fromstring(omml_xml_string)
            print("âœ… LLMè½¬è¯‘æˆåŠŸå¹¶å·²é€šè¿‡XMLéªŒè¯ã€‚")
            return omml_xml_string
        except etree.XMLSyntaxError as e:
            print(f"âŒ LLMè¿”å›çš„ä¸æ˜¯æœ‰æ•ˆçš„XMLï¼ŒéªŒè¯å¤±è´¥: {e}")
            print(f"æ”¶åˆ°çš„å†…å®¹: {omml_xml_string}")
            return None

    except requests.exceptions.RequestException as e:
        print(f"âŒ è°ƒç”¨LLMè¿›è¡Œå…¬å¼è½¬è¯‘å¤±è´¥: {e}")
        return None


# â˜…â˜…â˜… å·²æ·»åŠ è¯¦ç»†æ§åˆ¶å°æ—¥å¿— â˜…â˜…â˜…
def split_command_into_chunks(user_command: str, max_chunks: int = 5) -> tuple[list[str], str]:
    """
    ã€åŠ¨æ€åˆ†ç‰‡æ ¸å¿ƒå®ç°ã€‘
    å°†ç”¨æˆ·çš„é•¿æŒ‡ä»¤åˆ†å‰²æˆæ›´å°çš„ã€ç¬¦åˆé€»è¾‘çš„ä»»åŠ¡å—ã€‚
    """
    log_messages = []

    print("\n" + "=" * 20 + " 1. å¼€å§‹æ™ºèƒ½æŒ‡ä»¤åˆ†å‰² " + "=" * 20)

    # ç²—åˆ†
    logical_units = re.split(r'\n\s*\n+', user_command.strip())
    logical_units = [unit.strip() for unit in logical_units if unit.strip()]
    total_units = len(logical_units)

    print(f"[æ§åˆ¶å°] ç²—ç²’åº¦åˆ†å‰²ï¼šæ‰¾åˆ° {total_units} ä¸ªé€»è¾‘å•å…ƒã€‚")
    log_messages.append(f"ğŸ§  æŒ‡ä»¤è¢«åˆæ­¥åˆ†è§£ä¸º {total_units} ä¸ªé€»è¾‘å•å…ƒã€‚")

    if total_units <= max_chunks:
        print(f"[æ§åˆ¶å°] é€»è¾‘å•å…ƒæ•° ({total_units}) <= æœ€å¤§åˆ†å—æ•° ({max_chunks})ï¼Œæ— éœ€åˆå¹¶ã€‚")
        log_messages.append(f"  - å•å…ƒæ•° ({total_units}) ä¸è¶…è¿‡æœ€å¤§åˆ†å—æ•° ({max_chunks})ï¼Œæ— éœ€åˆå¹¶ã€‚")
        print("=" * 62 + "\n")
        return logical_units, "\n".join(log_messages)

    # ç²¾åˆ
    print(f"[æ§åˆ¶å°] é€»è¾‘å•å…ƒæ•° ({total_units}) > æœ€å¤§åˆ†å—æ•° ({max_chunks})ï¼Œå¼€å§‹æ™ºèƒ½åˆ†ç»„ã€‚")
    log_messages.append(f"  - å•å…ƒæ•° ({total_units}) è¶…è¿‡æœ€å¤§åˆ†å—æ•° ({max_chunks})ï¼Œå¼€å§‹æ™ºèƒ½åˆ†ç»„...")

    units_per_chunk = math.ceil(total_units / max_chunks)
    print(f"[æ§åˆ¶å°] è®¡ç®—å¾—å‡ºï¼šæ¯ä¸ªä»»åŠ¡å—åº”åŒ…å«çº¦ {units_per_chunk} ä¸ªé€»è¾‘å•å…ƒã€‚")
    log_messages.append(f"  - è®¡ç®—å¾—å‡ºï¼šæ¯ä¸ªä»»åŠ¡å—åº”åŒ…å«çº¦ {units_per_chunk} ä¸ªé€»è¾‘å•å…ƒã€‚")

    final_chunks = []
    for i in range(0, total_units, units_per_chunk):
        group = logical_units[i:i + units_per_chunk]
        combined_chunk = "\n".join(group)
        final_chunks.append(combined_chunk)

    print(f"[æ§åˆ¶å°] æˆåŠŸå°† {total_units} ä¸ªé€»è¾‘å•å…ƒåˆå¹¶ä¸º {len(final_chunks)} ä¸ªæœ€ç»ˆä»»åŠ¡å—ã€‚")
    log_messages.append(f"âœ… æˆåŠŸå°† {total_units} ä¸ªé€»è¾‘å•å…ƒåˆå¹¶ä¸º {len(final_chunks)} ä¸ªæœ€ç»ˆä»»åŠ¡å—ã€‚")
    print("=" * 62 + "\n")

    return final_chunks, "\n".join(log_messages)


# â˜…â˜…â˜… å·²æ·»åŠ è¯¦ç»†æ§åˆ¶å°æ—¥å¿— â˜…â˜…â˜…
def parse_natural_language_to_json(user_command: str) -> tuple[dict | None, str]:
    """
    å°†ç”¨æˆ·çš„è‡ªç„¶è¯­è¨€æŒ‡ä»¤åˆ†å—å‘é€ç»™LLMï¼Œå¹¶è¿”å›æœ€ç»ˆçš„JSONå’Œè¯¦ç»†çš„å¤„ç†æ—¥å¿—ã€‚
    """
    log_messages = []
    try:
        with open(SYSTEM_PROMPT_FILE, 'r', encoding='utf-8') as f:
            system_prompt = f.read()
    except FileNotFoundError:
        error_msg = f"âŒ é”™è¯¯ï¼šç³»ç»Ÿæç¤ºæ–‡ä»¶æœªæ‰¾åˆ° -> {SYSTEM_PROMPT_FILE}"
        print(f"[æ§åˆ¶å°] {error_msg}")
        return None, error_msg

    chunks, split_log = split_command_into_chunks(user_command, max_chunks=5)
    log_messages.append(split_log)

    aggregated_document_data = {"elements": []}

    print("=" * 20 + " 2. å¼€å§‹å¾ªç¯å¤„ç†ä»»åŠ¡å— " + "=" * 20)
    log_messages.append(f"\n--- å¼€å§‹é€ä¸€è°ƒç”¨AIè§£æå™¨å¤„ç† {len(chunks)} ä¸ªä»»åŠ¡å— ---")

    for i, chunk in enumerate(chunks):
        print(f"\n--- [æ§åˆ¶å°] æ­£åœ¨å¤„ç†ç¬¬ {i + 1}/{len(chunks)} ä¸ªä»»åŠ¡å— ---")
        log_messages.append(f"\n--- æ­£åœ¨å¤„ç†ç¬¬ {i + 1}/{len(chunks)} ä¸ªä»»åŠ¡å— ---")

        print(f"[æ§åˆ¶å°] ä»»åŠ¡å—å†…å®¹:\n---\n{chunk}\n---")
        log_messages.append(f"ğŸ“„ æŒ‡ä»¤å†…å®¹:\n---\n{chunk}\n---")

        # æ„å»ºä¸Šä¸‹æ–‡æ„ŸçŸ¥çš„ Prompt
        context_summary = f"So far, {len(aggregated_document_data.get('elements', []))} elements have been generated."
        chunk_user_prompt = f"""
        This is part {i + 1} of a multi-part command.
        The user's command for THIS part is: "{chunk}"
        CONTEXT: {context_summary}. 
        Please generate the JSON structure ONLY for the command in THIS part. Do not repeat or re-generate previous elements."""

        print("[æ§åˆ¶å°] ä¸ºæ­¤ä»»åŠ¡å—ç”Ÿæˆçš„ User Prompt:")
        print(chunk_user_prompt)

        payload = {
            "model": MODEL_NAME,
            "messages": [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": chunk_user_prompt}
            ],
            "format": "json",
            "stream": False
        }

        try:
            print("[æ§åˆ¶å°] æ­£åœ¨å‘ Ollama API å‘é€è¯·æ±‚...")
            response = requests.post(OLLAMA_API_URL, json=payload, timeout=60)
            response.raise_for_status()
            response_data = response.json()
            message_content = response_data.get('message', {}).get('content')
            print("[æ§åˆ¶å°] å·²æ”¶åˆ° AI å“åº”ã€‚")

            if not message_content:
                error_msg = f"âŒ é”™è¯¯ï¼šç¬¬ {i + 1} ä¸ªå—çš„AIå“åº”ä¸­æ‰¾ä¸åˆ°å†…å®¹ã€‚"
                print(f"[æ§åˆ¶å°] {error_msg}")
                log_messages.append(error_msg)
                return None, "\n".join(log_messages)

            chunk_json = json.loads(message_content)

            print(f"[æ§åˆ¶å°] AIä¸ºå— {i + 1} è¿”å›çš„JSONç‰‡æ®µ:")
            print(json.dumps(chunk_json, indent=2, ensure_ascii=False))
            log_messages.append(f"ğŸ¤– AIä¸ºå— {i + 1} è¿”å›çš„JSONç‰‡æ®µ:")
            log_messages.append(json.dumps(chunk_json, indent=2, ensure_ascii=False))

            # èšåˆ JSON
            new_elements = chunk_json.get('elements', [])
            if new_elements:
                if 'elements' not in aggregated_document_data:
                    aggregated_document_data['elements'] = []
                aggregated_document_data['elements'].extend(new_elements)
                print(f"[æ§åˆ¶å°] æˆåŠŸèšåˆ {len(new_elements)} ä¸ªæ–°å…ƒç´ ã€‚")
                log_messages.append(f"âœ… æˆåŠŸèšåˆ {len(new_elements)} ä¸ªæ–°å…ƒç´ ã€‚")

            if 'page_setup' in chunk_json:
                if 'page_setup' not in aggregated_document_data:
                    aggregated_document_data['page_setup'] = {}
                aggregated_document_data['page_setup'].update(chunk_json['page_setup'])
                print("[æ§åˆ¶å°] å·²æ›´æ–°é¡µé¢è®¾ç½®ã€‚")
                log_messages.append("âœ… å·²æ›´æ–°é¡µé¢è®¾ç½®ã€‚")

        except requests.exceptions.RequestException as e:
            error_msg = f"âŒ é”™è¯¯ï¼šåœ¨å¤„ç†ç¬¬ {i + 1} ä¸ªå—æ—¶è¿æ¥Ollama APIå¤±è´¥ -> {e}"
            print(f"[æ§åˆ¶å°] {error_msg}")
            log_messages.append(error_msg)
            return None, "\n".join(log_messages)
        except json.JSONDecodeError as e:
            error_msg = f"âŒ é”™è¯¯ï¼šåœ¨å¤„ç†ç¬¬ {i + 1} ä¸ªå—æ—¶AIè¿”å›çš„ä¸æ˜¯æœ‰æ•ˆçš„JSONæ ¼å¼ -> {e}"
            print(f"[æ§åˆ¶å°] {error_msg}")
            log_messages.append(error_msg)
            print(f"[æ§åˆ¶å°] æ”¶åˆ°çš„åŸå§‹å“åº”å†…å®¹: {message_content}")
            log_messages.append(f"æ”¶åˆ°çš„å†…å®¹: {message_content}")
            return None, "\n".join(log_messages)

    print("\n" + "=" * 20 + " 3. æ‰€æœ‰ä»»åŠ¡å—å¤„ç†å®Œæ¯• " + "=" * 20)
    log_messages.append("\nâœ… æ‰€æœ‰ä»»åŠ¡å—å¤„ç†å®Œæ¯•ï¼ŒAIè§£ææˆåŠŸï¼")
    return aggregated_document_data, "\n".join(log_messages)