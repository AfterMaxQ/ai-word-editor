# src/ai_parser.py

import json
import math
import re
import requests
from lxml import etree

# å¯¼å…¥æˆ‘ä»¬è‡ªå·±çš„åº“å’Œ Pydantic æ¨¡å‹
from .ollama_pydantic import create
from .schemas import DocumentModel

"""NAME                     ID              SIZE      MODIFIED
deepseek-coder-v2:16b    63fb193b3a9b    8.9 GB    6 hours ago
qwen3-vl:235b-cloud      86b3322ec200    -         7 hours ago
gpt-oss:120b-cloud       569662207105    -         24 hours ago
gpt-oss:20b              17052f91a42e    13 GB     24 hours ago
qwen3-vl:8b              901cae732162    6.1 GB    24 hours ago
qwen3-vl:4b              1343d82ebee3    3.3 GB    24 hours ago
qwen2.5-coder:14b        9ec8897f747e    9.0 GB    2 days ago
qwen2.5-coder:7b         dae161e27b0e    4.7 GB    2 days ago
deepseek-r1:14b          c333b7232bdb    9.0 GB    2 days ago
deepseek-r1:7b           755ced02ce7b    4.7 GB    2 days ago
llama3:8b                365c0bd3c000    4.7 GB    2 days ago"""

# --- å¸¸é‡å®šä¹‰ ---
MODEL_NAME = "deepseek-coder-v2:16b"
POLISH_MODEL_NAME = "deepseek-coder-v2:16b"
SYSTEM_PROMPT_FILE = "prompts/system_prompt.txt"
LATEX_PROMPT_FILE = "prompts/prompt_for_latex_convert.txt"
OLLAMA_API_URL = "http://localhost:11434/api/chat"
POLISH_PROMPT_FILE = "prompts/prompt_for_polishing.txt"


def translate_latex_to_omml_llm(latex_string: str) -> str | None:
    """
    (æ­¤å‡½æ•°ä¿æŒä¸å˜)
    ä½¿ç”¨LLMå°†LaTeXå­—ç¬¦ä¸²è½¬æ¢ä¸ºOMMLã€‚è¿™æ˜¯ä¸€ä¸ªç‹¬ç«‹çš„è¾…åŠ©å‡½æ•°ã€‚
    """
    print(f"ğŸ¤– å°è¯•ä½¿ç”¨LLMè½¬è¯‘LaTeX: {latex_string}")
    try:
        with open(LATEX_PROMPT_FILE, 'r', encoding='utf-8') as f:
            system_prompt = f.read()
    except FileNotFoundError:
        print(f"âŒ è‡´å‘½é”™è¯¯: æ— æ³•æ‰¾åˆ°LaTeXè½¬æ¢æç¤ºè¯æ–‡ä»¶ -> {LATEX_PROMPT_FILE}")
        return None

    user_prompt = f"""
Convert the following LaTeX formula into a centered OMML `<m:oMathPara>` XML block.
LaTeX Input: `{latex_string}`
Alignment: center
"""

    payload = {
        "model": MODEL_NAME,
        "messages": [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt}
        ],
        "stream": False
    }

    try:
        response = requests.post(OLLAMA_API_URL, json=payload, timeout=5000)
        response.raise_for_status()
        response_data = response.json()
        omml_xml_string = response_data.get('message', {}).get('content')

        if not omml_xml_string:
            print("âŒ LLMè¿”å›å†…å®¹ä¸ºç©ºã€‚")
            return None

        try:
            # æ¸…ç†LLMå¯èƒ½è¿”å›çš„markdownä»£ç å—æ ‡è®°
            omml_xml_string = re.sub(r'^```xml\s*|\s*```$', '', omml_xml_string, flags=re.MULTILINE).strip()
            etree.fromstring(omml_xml_string)
            print("âœ… LLMè½¬è¯‘æˆåŠŸå¹¶å·²é€šè¿‡XMLéªŒè¯ã€‚")
            return omml_xml_string
        except etree.XMLSyntaxError as e:
            print(f"âŒ LLMè¿”å›çš„ä¸æ˜¯æœ‰æ•ˆçš„XMLï¼ŒéªŒè¯å¤±è´¥: {e}")
            print(f"æ”¶åˆ°çš„å†…å®¹: {omml_xml_string}")
            return None

    except requests.exceptions.RequestException as e:
        print(f"âŒ è°ƒç”¨LLMè¿›è¡Œå…¬å¼è½¬è¯‘å¤±è´¥: {e}")
        return None


def split_command_into_chunks(user_command: str, max_chunks: int = 30) -> tuple[list[str], str]:
    """
    (æ­¤å‡½æ•°ä¿æŒä¸å˜)
    ã€åŠ¨æ€åˆ†ç‰‡æ ¸å¿ƒå®ç°ã€‘
    å°†ç”¨æˆ·çš„é•¿æŒ‡ä»¤åˆ†å‰²æˆæ›´å°çš„ã€ç¬¦åˆé€»è¾‘çš„ä»»åŠ¡å—ã€‚
    """
    log_messages = []

    print("\n" + "=" * 20 + " 1. å¼€å§‹æ™ºèƒ½æŒ‡ä»¤åˆ†å‰² " + "=" * 20)

    # ç²—åˆ†ï¼šæ ¹æ®ä¸€ä¸ªæˆ–å¤šä¸ªç©ºè¡Œæ¥åˆ†å‰²
    logical_units = re.split(r'\n\s*\n+', user_command.strip())
    logical_units = [unit.strip() for unit in logical_units if unit.strip()]
    total_units = len(logical_units)

    print(f"[æ§åˆ¶å°] ç²—ç²’åº¦åˆ†å‰²ï¼šæ‰¾åˆ° {total_units} ä¸ªé€»è¾‘å•å…ƒã€‚")
    log_messages.append(f"ğŸ§  æŒ‡ä»¤è¢«åˆæ­¥åˆ†è§£ä¸º {total_units} ä¸ªé€»è¾‘å•å…ƒã€‚")

    # å¦‚æœå•å…ƒæ•°åœ¨é™åˆ¶å†…ï¼Œç›´æ¥è¿”å›ï¼Œæ— éœ€åˆå¹¶
    if total_units <= max_chunks:
        print(f"[æ§åˆ¶å°] é€»è¾‘å•å…ƒæ•° ({total_units}) <= æœ€å¤§åˆ†å—æ•° ({max_chunks})ï¼Œæ— éœ€åˆå¹¶ã€‚")
        log_messages.append(f"  - å•å…ƒæ•° ({total_units}) ä¸è¶…è¿‡æœ€å¤§åˆ†å—æ•° ({max_chunks})ï¼Œæ— éœ€åˆå¹¶ã€‚")
        print("=" * 62 + "\n")
        return logical_units, "\n".join(log_messages)

    # ç²¾åˆï¼šå¦‚æœå•å…ƒæ•°è¿‡å¤šï¼Œåˆ™è¿›è¡Œæ™ºèƒ½åˆ†ç»„
    print(f"[æ§åˆ¶å°] é€»è¾‘å•å…ƒæ•° ({total_units}) > æœ€å¤§åˆ†å—æ•° ({max_chunks})ï¼Œå¼€å§‹æ™ºèƒ½åˆ†ç»„ã€‚")
    log_messages.append(f"  - å•å…ƒæ•° ({total_units}) è¶…è¿‡æœ€å¤§åˆ†å—æ•° ({max_chunks})ï¼Œå¼€å§‹æ™ºèƒ½åˆ†ç»„...")

    units_per_chunk = math.ceil(total_units / max_chunks)
    print(f"[æ§åˆ¶å°] è®¡ç®—å¾—å‡ºï¼šæ¯ä¸ªä»»åŠ¡å—åº”åŒ…å«çº¦ {units_per_chunk} ä¸ªé€»è¾‘å•å…ƒã€‚")
    log_messages.append(f"  - è®¡ç®—å¾—å‡ºï¼šæ¯ä¸ªä»»åŠ¡å—åº”åŒ…å«çº¦ {units_per_chunk} ä¸ªé€»è¾‘å•å…ƒã€‚")

    final_chunks = []
    for i in range(0, total_units, units_per_chunk):
        group = logical_units[i:i + units_per_chunk]
        combined_chunk = "\n".join(group)
        final_chunks.append(combined_chunk)

    print(f"[æ§åˆ¶å°] æˆåŠŸå°† {total_units} ä¸ªé€»è¾‘å•å…ƒåˆå¹¶ä¸º {len(final_chunks)} ä¸ªæœ€ç»ˆä»»åŠ¡å—ã€‚")
    log_messages.append(f"âœ… æˆåŠŸå°† {total_units} ä¸ªé€»è¾‘å•å…ƒåˆå¹¶ä¸º {len(final_chunks)} ä¸ªæœ€ç»ˆä»»åŠ¡å—ã€‚")
    print("=" * 62 + "\n")

    return final_chunks, "\n".join(log_messages)


def parse_natural_language_to_json(user_command: str) -> tuple[dict | None, str]:
    """
    å°†ç”¨æˆ·çš„è‡ªç„¶è¯­è¨€æŒ‡ä»¤åˆ†å—å‘é€ç»™LLMï¼Œå¹¶è¿”å›æœ€ç»ˆçš„JSONå’Œè¯¦ç»†çš„å¤„ç†æ—¥å¿—ã€‚
    (å·²é‡æ„ä¸ºä½¿ç”¨è‡ªç ”çš„ ollama_pydantic åº“)
    """
    log_messages = []
    try:
        with open(SYSTEM_PROMPT_FILE, 'r', encoding='utf-8') as f:
            system_prompt = f.read()
    except FileNotFoundError:
        error_msg = f"âŒ é”™è¯¯ï¼šç³»ç»Ÿæç¤ºæ–‡ä»¶æœªæ‰¾åˆ° -> {SYSTEM_PROMPT_FILE}"
        print(f"[æ§åˆ¶å°] {error_msg}")
        return None, error_msg

    chunks, split_log = split_command_into_chunks(user_command, max_chunks=30)
    log_messages.append(split_log)

    aggregated_document_data = {"sections": []}

    print("=" * 20 + " 2. å¼€å§‹å¾ªç¯å¤„ç†ä»»åŠ¡å— " + "=" * 20)
    log_messages.append(f"\n--- å¼€å§‹é€ä¸€è°ƒç”¨AIè§£æå™¨å¤„ç† {len(chunks)} ä¸ªä»»åŠ¡å— ---")

    for i, chunk in enumerate(chunks):
        print(f"\n--- [æ§åˆ¶å°] æ­£åœ¨å¤„ç†ç¬¬ {i + 1}/{len(chunks)} ä¸ªä»»åŠ¡å— ---")
        log_messages.append(f"\n--- æ­£åœ¨å¤„ç†ç¬¬ {i + 1}/{len(chunks)} ä¸ªä»»åŠ¡å— ---")
        print(f"[æ§åˆ¶å°] ä»»åŠ¡å—å†…å®¹:\n---\n{chunk}\n---")
        log_messages.append(f"ğŸ“„ æŒ‡ä»¤å†…å®¹:\n---\n{chunk}\n---")

        context_summary = f"So far, {len(aggregated_document_data.get('sections', []))} sections have been generated."
        chunk_user_prompt = f"""
CRITICAL INSTRUCTION: You are a component in a larger system. 
Your SOLE task is to convert the user's command for THIS specific part into a JSON structure that conforms to the Pydantic model.
You MUST NOT add any content, text, or elements not explicitly requested in the command below.
This is part {i + 1} of a multi-part command.
The user's command for THIS part is:
---
{chunk}
---
CONTEXT: {context_summary}.

YOUR TASK:
1.  Analyze the command for THIS part ONLY.
2.  If the command is purely instructional, transitional (e.g., "Next, do the following"), or a summary, and contains NO concrete content to add to the document, you MUST return a valid JSON that will result in an empty Pydantic model (e.g., {{}} or {{"sections": []}}).
3.  Otherwise, generate the JSON structure strictly for the elements described in THIS part. Do not hallucinate or create extra content.
"""
        print("[æ§åˆ¶å°] ä¸ºæ­¤ä»»åŠ¡å—ç”Ÿæˆçš„ User Prompt:")
        print(chunk_user_prompt)

        # â–¼â–¼â–¼ã€æ ¸å¿ƒè°ƒç”¨é€»è¾‘ã€‘â–¼â–¼â–¼
        # è°ƒç”¨æˆ‘ä»¬è‡ªå·±çš„ `ollama_pydantic.create` å‡½æ•°
        chunk_model = create(
            model=MODEL_NAME,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": chunk_user_prompt}
            ],
            response_model=DocumentModel,
            max_retries=2,
        )
        # â–²â–²â–²ã€æ ¸å¿ƒè°ƒç”¨é€»è¾‘ã€‘â–²â–²â–²

        if chunk_model is None:
            # å¦‚æœ `create` å‡½æ•°åœ¨æ‰€æœ‰é‡è¯•åè¿”å› Noneï¼Œè¯´æ˜å½»åº•å¤±è´¥
            error_msg = f"âŒ è‡´å‘½é”™è¯¯ï¼šåœ¨å¤„ç†ç¬¬ {i + 1} ä¸ªå—æ—¶ï¼ŒAIåœ¨å¤šæ¬¡å°è¯•åä»æ— æ³•ç”Ÿæˆæœ‰æ•ˆJSONã€‚"
            print(f"[æ§åˆ¶å°] {error_msg}")
            log_messages.append(error_msg)
            return None, "\n".join(log_messages)

        # å°†è¿”å›çš„ Pydantic æ¨¡å‹å®ä¾‹è½¬æ¢ä¸ºå­—å…¸ï¼Œç”¨äºåç»­çš„èšåˆæ“ä½œ
        chunk_json = chunk_model.model_dump(exclude_unset=True)

        print(f"[æ§åˆ¶å°] AIä¸ºå— {i + 1} è¿”å›çš„å·²éªŒè¯JSONç‰‡æ®µ:")
        print(json.dumps(chunk_json, indent=2, ensure_ascii=False))
        log_messages.append(f"ğŸ¤– AIä¸ºå— {i + 1} è¿”å›çš„å·²éªŒè¯JSONç‰‡æ®µ:")
        log_messages.append(json.dumps(chunk_json, indent=2, ensure_ascii=False))

        # èšåˆé€»è¾‘
        new_sections = chunk_json.get('sections', [])
        if new_sections:
            if 'sections' not in aggregated_document_data:
                aggregated_document_data['sections'] = []
            aggregated_document_data['sections'].extend(new_sections)
            print(f"[æ§åˆ¶å°] æˆåŠŸèšåˆ {len(new_sections)} ä¸ªæ–°èŠ‚(section)ã€‚")
            log_messages.append(f"âœ… æˆåŠŸèšåˆ {len(new_sections)} ä¸ªæ–°èŠ‚(section)ã€‚")

        if 'page_setup' in chunk_json:
            if 'page_setup' not in aggregated_document_data:
                aggregated_document_data['page_setup'] = {}
            aggregated_document_data['page_setup'].update(chunk_json['page_setup'])
            print("[æ§åˆ¶å°] å·²æ›´æ–°é¡µé¢è®¾ç½®ã€‚")
            log_messages.append("âœ… å·²æ›´æ–°é¡µé¢è®¾ç½®ã€‚")

    print("\n" + "=" * 20 + " 3. æ‰€æœ‰ä»»åŠ¡å—å¤„ç†å®Œæ¯• " + "=" * 20)
    log_messages.append("\nâœ… æ‰€æœ‰ä»»åŠ¡å—å¤„ç†å®Œæ¯•ï¼ŒAIè§£ææˆåŠŸï¼")
    return aggregated_document_data, "\n".join(log_messages)

def polish_user_prompt_llm(user_command: str) -> str | None:
    """
    ä½¿ç”¨LLMå°†ç”¨æˆ·è¾“å…¥çš„æ¨¡ç³ŠæŒ‡ä»¤æ¶¦è‰²æˆæ¸…æ™°ã€ç»“æ„åŒ–çš„æŒ‡ä»¤ã€‚
    """
    print(f" polishing user prompt: {user_command}")
    try:
        with open(POLISH_PROMPT_FILE, 'r', encoding='utf-8') as f:
            system_prompt = f.read()
    except FileNotFoundError:
        print(f"âŒ è‡´å‘½é”™è¯¯: æ— æ³•æ‰¾åˆ°æŒ‡ä»¤æ¶¦è‰²æç¤ºè¯æ–‡ä»¶ -> {POLISH_PROMPT_FILE}")
        return None

    # å°†ç”¨æˆ·çš„åŸå§‹æŒ‡ä»¤é™„åŠ åˆ°ç³»ç»Ÿæç¤ºè¯çš„æœ«å°¾
    full_prompt = f"{system_prompt}\n{user_command}"

    payload = {
        "model": POLISH_MODEL_NAME, # å¯ä»¥ä¸ºè¿™ä¸ªä»»åŠ¡é€‰æ‹©ä¸€ä¸ªä¸åŒçš„ã€æ›´æ“…é•¿åˆ›æ„çš„æ¨¡å‹
        "messages": [
            {"role": "system", "content": "You are a helpful assistant that rewrites user commands."}, # ç®€å•çš„ç³»ç»Ÿè§’è‰²
            {"role": "user", "content": full_prompt}
        ],
        "stream": False
    }

    try:
        response = requests.post(OLLAMA_API_URL, json=payload, timeout=5000)
        response.raise_for_status()
        response_data = response.json()
        polished_command = response_data.get('message', {}).get('content')

        if not polished_command:
            print("âŒ LLMè¿”å›çš„æ¶¦è‰²å†…å®¹ä¸ºç©ºã€‚")
            return user_command # å¦‚æœå¤±è´¥ï¼Œè¿”å›åŸå§‹æŒ‡ä»¤

        # ç®€å•çš„æ¸…ç†ï¼Œç§»é™¤å¯èƒ½çš„å‰åç©ºè¡Œ
        return polished_command.strip()

    except requests.exceptions.RequestException as e:
        print(f"âŒ è°ƒç”¨LLMè¿›è¡ŒæŒ‡ä»¤æ¶¦è‰²å¤±è´¥: {e}")
        return user_command # å¦‚æœå¤±è´¥ï¼Œè¿”å›åŸå§‹æŒ‡ä»¤